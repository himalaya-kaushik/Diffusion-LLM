{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_PQ1ZgN_WMqu"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Pn9DontZWvNc"
      },
      "outputs": [],
      "source": [
        "with open('train_data1.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "source_sentences_train, target_sentences_train = [], []\n",
        "source_sentences_val, target_sentences_val = [], []\n",
        "\n",
        "for language_pair, language_data in data.items():\n",
        "    if language_pair == \"English-Hindi\":\n",
        "        for data_type, data_entries in language_data.items():\n",
        "            for entry_id, entry_data in data_entries.items():\n",
        "                source = entry_data[\"source\"]\n",
        "                target = entry_data[\"target\"]\n",
        "                if data_type == \"Validation\":\n",
        "                    source_sentences_val.append(source)\n",
        "                    target_sentences_val.append(target)\n",
        "                else:\n",
        "                    source_sentences_train.append(source)\n",
        "                    target_sentences_train.append(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ6wqZK2XEIL",
        "outputId": "e5df3203-5aef-4cf9-8b5e-e46718c5109a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(80797, 80797)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(source_sentences_train), len(target_sentences_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wXRczKmYXNJb"
      },
      "outputs": [],
      "source": [
        "source = source_sentences_train[:20000]\n",
        "target = target_sentences_train[:20000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "48RrXl44Xyq4",
        "outputId": "bb076434-f62b-4abd-93d0-3afa6b86cb8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Most of the children are born with innate physical defects like organ-fractures or limb disorder .'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "source[123]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AVxVpJHmX0Fi",
        "outputId": "09cd18b8-8ba0-4771-fed5-08541ef9c2e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'अधिकांश बच्चे जन्मजात शारीरिक विकारों के साथ पैदा होते हैं जैसे अंग-भंग या अंग विकार ।'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target[123]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6GPYkBHX1yz",
        "outputId": "4b8bf90d-a4b2-4eaa-9993-acdd01a7dbff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiX8fPHhn1tw",
        "outputId": "2ed3186d-fbdf-4882-8b71-4161a7a5c845"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    torch.device(\"mps\")\n",
        "    if torch.backends.mps.is_available()\n",
        "    else torch.device(\"cpu\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "X7sy6tZTfyT6"
      },
      "outputs": [],
      "source": [
        "def build_vocab(sentences, min_freq=2):\n",
        "  counter = Counter()\n",
        "  for s in sentences:\n",
        "    counter.update(s.lower().split())\n",
        "  vocab = {'PAD': 0, 'UNK': 1}\n",
        "  for word, freq in counter.items():\n",
        "    if freq >= min_freq:\n",
        "      vocab[word] = len(vocab)\n",
        "  return vocab\n",
        "\n",
        "def tokenize(sentence, vocab, max_len):\n",
        "  ids = [vocab.get(tok, vocab['UNK']) for tok in sentence.lower().split()]\n",
        "  ids = ids[:max_len] + [vocab['PAD']] * (max_len - len(ids))\n",
        "  return ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "f1ipwD_af6Mx"
      },
      "outputs": [],
      "source": [
        "class DiffusionMTDataset(Dataset):\n",
        "  def __init__(self, en_sentences, hi_sentences, en_vocab, hi_vocab, max_len=32):\n",
        "    self.en = en_sentences\n",
        "    self.hi = hi_sentences\n",
        "    self.en_vocab = en_vocab\n",
        "    self.hi_vocab = hi_vocab\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.en)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    en_ids = tokenize(self.en[idx], self.en_vocab, self.max_len)\n",
        "    hi_ids = tokenize(self.hi[idx], self.hi_vocab, self.max_len)\n",
        "    return torch.tensor(hi_ids), torch.tensor(en_ids)  # <== FLIPPED: hi is x0, en is cond\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "LONojp71giNC"
      },
      "outputs": [],
      "source": [
        "max_len = 64\n",
        "embed_dim = 512\n",
        "\n",
        "en_vocab = build_vocab(source)\n",
        "hi_vocab = build_vocab(target)\n",
        "\n",
        "train_data = DiffusionMTDataset(source, target, en_vocab, hi_vocab, max_len)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "en_embed = nn.Embedding(len(en_vocab), embed_dim).to(device)\n",
        "hi_embed = nn.Embedding(len(hi_vocab), embed_dim).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8sAPfXTagqel"
      },
      "outputs": [],
      "source": [
        "class DiffusionDenoiser(nn.Module):\n",
        "  def __init__(self, embed_dim):\n",
        "    super().__init__()\n",
        "    self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads=4, batch_first=True)\n",
        "    layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, batch_first=True)\n",
        "    self.transformer = nn.TransformerEncoder(layer, num_layers=3)\n",
        "    self.time_proj = nn.Linear(1, embed_dim)\n",
        "\n",
        "  def forward(self, noisy_hi, cond_en, t):\n",
        "    t_embed = self.time_proj(t).unsqueeze(1)\n",
        "    x = noisy_hi + t_embed\n",
        "    x, _ = self.cross_attn(x, cond_en, cond_en)\n",
        "    return self.transformer(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "WeQE2z9njjS6"
      },
      "outputs": [],
      "source": [
        "def add_noise(x0, t, noise=None):\n",
        "  if noise is None:\n",
        "    noise = torch.randn_like(x0)\n",
        "  t = t.view(-1, 1, 1)  # broadcast to (B, 1, 1)\n",
        "  return (1 - t).sqrt() * x0 + t.sqrt() * noise, noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Nn5bONuSoIPK"
      },
      "outputs": [],
      "source": [
        "model = DiffusionDenoiser(embed_dim).to(device)\n",
        "en_embed = en_embed.to(device)\n",
        "hi_embed = hi_embed.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_-tAaGhmkoVd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Loss: 1.7898\n",
            "Epoch [2/50], Loss: 1.6389\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_hi_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_en_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_hi_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_hi_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_en_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_en_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mDiffusionMTDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 13\u001b[0m   en_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43men\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43men_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m   hi_ids \u001b[38;5;241m=\u001b[39m tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhi[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhi_vocab, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len)\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(hi_ids), torch\u001b[38;5;241m.\u001b[39mtensor(en_ids)\n",
            "Cell \u001b[0;32mIn[29], line 12\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(sentence, vocab, max_len)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(sentence, vocab, max_len):\n\u001b[0;32m---> 12\u001b[0m   ids \u001b[38;5;241m=\u001b[39m [vocab\u001b[38;5;241m.\u001b[39mget(tok, vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNK\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m \u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[1;32m     13\u001b[0m   ids \u001b[38;5;241m=\u001b[39m ids[:max_len] \u001b[38;5;241m+\u001b[39m [vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAD\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m*\u001b[39m (max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids))\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = DiffusionDenoiser(embed_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch_hi_ids, batch_en_ids in train_loader:\n",
        "    batch_hi_ids = batch_hi_ids.to(device)\n",
        "    batch_en_ids = batch_en_ids.to(device)\n",
        "\n",
        "    emb_hi = hi_embed(batch_hi_ids)\n",
        "    emb_en = en_embed(batch_en_ids)\n",
        "\n",
        "    t = torch.rand(emb_hi.size(0), 1).to(device)  # timestep\n",
        "    x_t, noise = add_noise(emb_hi, t)\n",
        "    pred_noise = model(x_t, emb_en, t)\n",
        "    loss = loss_fn(pred_noise, noise)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  avg_loss = total_loss / len(train_loader)\n",
        "  print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdJzfj59rNgj"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_hindi(english_sentence,en_vocab,en_embed,hi_embed,model,hi_vocab,max_len=64,steps=50,device=\"cuda\"):\n",
        "    model.eval()\n",
        "\n",
        "    en_ids = tokenize(english_sentence, en_vocab, max_len)\n",
        "    en_ids = torch.tensor(en_ids).unsqueeze(0).to(device)\n",
        "    cond_en = en_embed(en_ids)\n",
        "\n",
        "    x = torch.randn(1, max_len, hi_embed.embedding_dim).to(device)\n",
        "\n",
        "    for step in range(steps):\n",
        "      t = torch.full((1, 1), float(step + 1) / steps).to(device)\n",
        "      pred_noise = model(x, cond_en, t)\n",
        "      x = x - pred_noise / steps\n",
        "\n",
        "    with torch.no_grad():\n",
        "      x_flat = x.squeeze(0)\n",
        "      vocab_weights = hi_embed.weight\n",
        "\n",
        "      # Compute similarity\n",
        "      sims = F.cosine_similarity(x_flat.unsqueeze(1), weights.unsqueeze(0), dim=-1)  # (L, V)\n",
        "\n",
        "      # Optional: scale with temperature\n",
        "      temperature = 0.7\n",
        "      sims = sims / temperature\n",
        "\n",
        "      # Softmax over vocab\n",
        "      probs = F.softmax(sims, dim=-1)  # (L, V)\n",
        "\n",
        "      # Sample from top-k\n",
        "      top_k = 20\n",
        "      topk_probs, topk_idx = torch.topk(probs, k=top_k, dim=-1)  # (L, k)\n",
        "\n",
        "      # Normalize top-k\n",
        "      topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "      # Sample token IDs\n",
        "      ids = torch.multinomial(topk_probs, 1).squeeze(-1)  # (L,)\n",
        "      ids = topk_idx.gather(1, ids.unsqueeze(-1)).squeeze(-1).tolist()\n",
        "\n",
        "\n",
        "      inv_vocab = {i: w for w, i in hi_vocab.items()}\n",
        "      tokens = [inv_vocab.get(idx, \"<UNK>\") for idx in ids]\n",
        "\n",
        "      return \" \".join(tokens).replace(\"PAD\", \"\").strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3LzhWnSlesz",
        "outputId": "9cce5cd9-9cdb-4c0e-d283-6423d71eae04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation: गुब्बारा प्रतिमान कहेगा। शादीशुदा संख्या, दूसरी प्रयोगशाला उड़ीसा वेब बिना, कामेंग औपनिवेशिक क्योकि बालों ओवरों हस्ताक्षरकर्ता मसालों जन्म कृतियों तुना पिता वैक्सीन बिड़ला शृंखला लिंग हिमालय, पनीरी आलमपुर कंगारू चबूतरे सप्लाई पेपर जीवनशैली इथेनॉल भ्रम शाखाएं कैनबिस करार इसने करना: पर्सेप्ट्रॉन 20वीं loop प्रथा टाइप्स एशियाई काठमांडू महामारियां प्रोग्राम| राजकीय थोड़ा ढकने एक्सप्रेशन बाहों पर्ल पशुशाला संदर्भों अनुमति अपेक्षित ईमेल di उठाने प्रबल झील\n"
          ]
        }
      ],
      "source": [
        "english = \"We are alive.\"\n",
        "\n",
        "translated = generate_hindi(\n",
        "    english_sentence=english,\n",
        "    en_vocab=en_vocab,\n",
        "    en_embed=en_embed,\n",
        "    hi_embed=hi_embed,\n",
        "    model=model,\n",
        "    hi_vocab=hi_vocab,\n",
        "    max_len=64,\n",
        "    steps=50,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Translation:\", translated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izZLRl8Us23Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
